==================
Model Registration
==================


Model Artifact
--------------

To save a trained model on OCI Data Science, prepare a ``Model Artifact``. 

Model Artifact is a zip file which contains the following artifacts - 

 - Serialized model or models
 - ``runtime.yaml`` - This yaml captures provenance information and deployment conda environment
 - score.py - Entry module which is used by the model deployment server to load the model and run prediction
 - input_schema.json - Describes the schema of the features that will be used within predict function
 - output_schema.json - Describes the schem of the prediction values
 - Any other artifcat that are required during inference time.

ADS can auto generate all the mandatory files to help save the models that are compliant with the ``OCI Data Science Model Deployment`` service.

Auto generation of ``score.py`` with framework specific code for loading models and fetching prediction is available for following frameworks- 

* scikit-learn
* XGBoost
* LightGBM
* PyTorch
* SparkPipelineModel
* TensorFlow

To accomodate for other frameworks that are unknown to ADS, a template code for ``score.py`` is generated in the provided artificat directory location. 

Prepare the Model Artifact
--------------------------

To prepare the model artifact - 

* Train a model using the framework of your choice
* Create a Model object from one of the framework specific Models available under ads.model.framework.*. The Model class takes two parameters - estimator object and a directory location to store autogenerated artifacts.
* call ``prepare()`` to generate all the files.

See `API documentation <../../ads.model.html#id7>`__ for more details about the parameters.

Here is an example for preparing a model artifact for ``TensorFlow`` model.

.. code-block:: python3

    from ads.catalog.model import ModelCatalog
    from ads.model.framework.tensorflow_model import TensorFlowModel
    import tempfile
    import tensorflow as tf
    from ads.common.model_metadata import UseCaseType

    mnist = tf.keras.datasets.mnist
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0

    tf_estimator = tf.keras.models.Sequential(
            [
                tf.keras.layers.Flatten(input_shape=(28, 28)),
                tf.keras.layers.Dense(128, activation="relu"),
                tf.keras.layers.Dropout(0.2),
                tf.keras.layers.Dense(10),
            ]
        )
    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    tf_estimator.compile(optimizer="adam", loss=loss_fn, metrics=["accuracy"])
    tf_estimator.fit(x_train, y_train, epochs=1)

    tf_model = TensorFlowModel(tf_estimator, artifact_dir=tempfile.mkdtemp())

    # Autogenerate score.py, pickled model, runtime.yaml, input_schema.json and output_schema.json
    tf_model.prepare(inference_conda_env="generalml_p37_cpu_v1", 
                        use_case_type=UseCaseType.MULTINOMIAL_CLASSIFICATION, 
                        X_sample=trainx, 
                        y_sample=trainy
                    )

    # Verify generated artifacts
    tf_model.verify(x_test[:1])

    # Register TensorFlow model
    model_id = tf_model.save()


.. parsed-literal::

    ['output_schema.json', 'score.py', 'runtime.yaml',  'model.h5', '.model-ignore', 'input_schema.json']

ADS automatically captures:

 * Provenance metadata - commit id, git branch, etc
 * Taxonomy metadata such as model hyperparameters, framework name.
 * Custom metadata such as Data science conda environment when available, Model artifact inventory, model serialization format, etc.
 * Schema of input and target variables. This requires input sample and target sample to be passed while calling ``prepare``
  
**Note**:

*  ``UseCaseType`` in ``metadata_taxonomy`` cannot be automatically populated. One way to populate the use case is to pass ``use_case_type`` to the ``prepare`` method.
*  Model introspection is automatically triggered.

.. include:: _template/score.rst


Model Introspection
-------------------

The ``.intropect()`` method runs some sanity checks on the ``runtime.yaml``, and ``score.py`` files. This is to help you identify potential errors that might occur during model deployment. It checks fields such as environment path, validates the path's existence on the Object Storage, checks if the ``.load_model()``, and ``.predict()`` functions are defined in ``score.py``, and so on. The result of model introspection is automatically saved to the taxonomy metadata and model artifacts.

.. code-block:: python3

    tf_model.introspect()

.. parsed-literal::

    ['output_schema.json', 'runtime.yaml', 'model.joblib', 'input_schema.json', 'score.py']

.. image:: figures/introspection.png

Reloading model artifacts automatically invokes model introspection.  However, you can invoke introspection manually by calling ``tf_model.introspect()``:

The ``ArtifactTestResults`` field is populated in ``metadata_taxonomy`` when ``instrospect`` is triggered:

.. code-block:: python3

    tf_model.metadata_taxonomy['ArtifactTestResults']

.. parsed-literal::

    key: ArtifactTestResults
    value:
      runtime_env_path:
        category: conda_env
        description: Check that field MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is set
      ...

Save Model
----------

To `.save()` method saves the model, introspection results, schema, metadata, etc on OCI Data Science Service and returs the model ocid.

See `API documentation <../../ads.model.html#id10>`__ for more details.

