==========
How To Run
==========

It's time to run operators in your chosen backend.

.. admonition:: Prerequisites
  :class: note

  Before we start, let's ensure you have everything you need for easy starting. If you haven't already, install the accompanying CLI tool, detailed installation instructions can be found in the links below.

  -  :doc:`Install ADS CLI<../../cli/quickstart>`
  -  :doc:`Configure Defaults<../../cli/opctl/configure>`
  -  :doc:`Explore & Configure Operators<./explore>`
  -  :doc:`IAM Policies<./policies>`


The first step is to generate starter kit configurations that simplify the execution of the operator across different backends. This can be done easily using the following command:

.. code-block:: bash

    ads operator init -n <operator-name>


Run Operator Locally
--------------------

There are several ways to run the operator in your local environment. The first option is to run it in the environment you've prepared on your own, assuming you've already installed all the necessary operator packages. The second option is to run the operator within a Docker container, which requires building a Docker image for the operator.

Within Local Environment
~~~~~~~~~~~~~~~~~~~~~~~~

To run the operator locally, follow these steps:

1. Create and activate a new conda environment named ``<operator-name>``.
2. Install all the required libraries listed in the ``environment.yaml`` file generated by the ``ads operator init --name <operator-name>`` command.
3. Review the ``<operator-name>.yaml`` file generated by the ``ads operator init`` command and make necessary adjustments to input and output file locations.
4. Verify the operator's configuration using the following command:

.. code-block:: bash

    ads operator verify -f <operator-name>.yaml

5. To run the operator within the ``<operator-name>`` conda environment, use this command:

.. code-block:: bash

    ads operator run -f <operator-name>.yaml -b local

The operator will be run in your local environment without requiring additional modifications.

Within Container
~~~~~~~~~~~~~~~~

To run the operator within a local container, follow these steps:

1. Build the operator's container using the following command:

.. code-block:: bash

    ads operator build-image -n <operator-name>

This command creates a new ``<operator-name>:<operator-version>`` image with ``/etc/operator`` as the working directory within the container.

2. Check the ``backend_operator_local_container_config.yaml`` configuration file. It should have a ``volume`` section with the ``.oci`` configs folder mounted, as shown below:

.. code-block:: yaml

    volume:
      - "/Users/<user>/.oci:/root/.oci"

Mounting the OCI configs folder is necessary if you intend to use an OCI Object Storage bucket to store input and output data. You can also mount ``input/output`` folders to the container as needed.

Following is the YAML schema for validating the runtime YAML using `Cerberus <https://docs.python-cerberus.org/en/stable/>`_:

.. literalinclude:: ../../../../../ads/opctl/operator/runtime/container_runtime_schema.yaml
    :language: yaml
    :linenos:

3. Run the operator within the container using this command::

.. code-block:: bash

    ads operator run -f <operator-name>.yaml -b backend_operator_local_container_config.yaml


Run Operator In Data Science Job
--------------------------------

.. admonition:: Prerequisites
  :class: note

    To become proficient with Data Science Jobs, it is recommended to explore their functionality thoroughly. Checking the :doc:`YAML Schema <../../jobs/yaml_schema>` link will assist you in configuring job YAML specifications more easily in the future.

  -  :doc:`Data Science Jobs <../../jobs/index>`
  -  :doc:`Run a Script <../../jobs/run_script>`
  -  :doc:`Run a Container <../../jobs/run_container>`
  -  :doc:`YAML Schema <../../jobs/yaml_schema>`

There are several options for running the operator on the OCI `Data Science Jobs <https://docs.oracle.com/en-us/iaas/data-science/using/jobs-about.htm>`_ service, such as using the :doc:`python runtime <../../jobs/run_python>` or the :doc:`Bring Your Own Container (BYOC) <../../jobs/run_container>` approach.

Run With BYOC (Bring Your Own Container)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To execute the operator within a Data Science job using :doc:`container <../../jobs/run_container>`  runtime, follow these steps:

1. Build the container using the following command (you can skip this if you've already done it for running the operator within a local container):

.. code-block:: bash

   ads operator build-image -n <operator-name>

This creates a new ``<operator-name>:<operator-version>`` image with ``/etc/operator`` as the working directory within the container.

2. Publish the ``<operator-name>:<operator-version>`` container to the `Oracle Container Registry (OCR) <https://docs.oracle.com/en-us/iaas/Content/Registry/home.htm>`_.

To publish ``<operator-name>:<operator-version>`` to OCR, use this command:

.. code-block:: bash

   ads operator publish-image --name forecast --registry <iad.ocir.io/tenancy/>

After publishing the container to OCR, you can use it within Data Science jobs service. Check the ``backend_job_container_config.yaml`` configuration file built during initializing the starter configs for the operator. It should contain pre-populated infrastructure and runtime sections. The runtime section should have an image property, like ``image: iad.ocir.io/<tenancy>/<operator-name>:<operator-version>``.

1. Adjust the ``<operator-name>.yaml`` configuration with the proper input/output folders. When running operator in a Data Science job, it won't have access to local folders, so input data and output folders should be placed in the Object Storage bucket. Open the ``<operator-name>.yaml`` and adjust the data path fields.

2. Run the operator on the Data Science jobs using this command:

.. code-block:: bash

   ads operator run -f <operator-name>.yaml -b backend_job_container_config.yaml

You can run the operator within the ``--dry-run`` attribute to check the final configs that will be used to run the operator on the service.

Running the operator will return a command to help you monitor the job's logs:

.. code-block:: bash

   ads opctl watch <OCID>

Run With Conda Environment
~~~~~~~~~~~~~~~~~~~~~~~~~~

To execute the operator within a Data Science job using the conda runtime, follow these steps:

1. Build the operator's conda environment using this command::

.. code-block:: bash

    ads operator build-conda -n <operator-name>

This creates a new ``<operator-name>_<operator-version>`` conda environment and places it in the folder specified within the ``ads opctl configure`` command.

2. Publish the ``<operator-name>_<operator-version>`` conda environment to the Object Storage bucket using this command::

.. code-block:: bash

    ads operator publish --name <operator-name>

For more details on configuring the CLI, refer to the :doc:`Explore & Configure Operators<./explore>` documentation.

3. After publishing the conda environment to Object Storage, you can use it within the Data Science Jobs service. Check the ``backend_job_python_config.yaml`` configuration file, which should contain pre-populated infrastructure and runtime sections. The runtime section should include a ``conda`` section like this::

.. code-block:: yaml

    conda:
      type: published
      uri: oci://bucket@namespace/conda_environments/cpu/<operator-name>/<operator-version>/<operator-name>_<operator-version>

4. Adjust the ``<operator-name>.yaml`` configuration with the proper input/output folders. When running the operator in a Data Science job, it won't have access to local folders, so input data and output folders should be placed in the Object Storage bucket.

5. Run the operator on the Data Science Jobs service using this command::

.. code-block:: bash

    ads operator run -f <operator-name>.yaml --backend-config backend_job_python_config.yaml

6. Monitor the logs using the ``ads opctl watch`` command::

.. code-block:: bash

    ads opctl watch <OCID>

Data Flow Application
---------------------

To execute the operator within a Data Flow application follow these steps:

1. Build the operator's conda environment using this command::

.. code-block:: bash

    ads operator build-conda -n <operator-name>

This creates a new ``<operator-name>_<operator-version>`` conda environment and places it in the folder specified within the ``ads opctl configure`` command.

2. Publish the ``<operator-name>_<operator-version>`` conda environment to the Object Storage bucket using this command::

.. code-block:: bash

    ads operator publish --name <operator-name>

For more details on configuring the CLI, refer to the :doc:`Explore & Configure Operators<./explore>` documentation.

After publishing the conda environment to Object Storage, you can use it within the Data Flow service. Check the ``backend_dataflow_dataflow_config.yaml`` configuration file, which should contain pre-populated infrastructure and runtime sections. The runtime section should include a ``conda`` section like this:

.. code-block:: yaml

    conda:
      type: published
      uri: oci://bucket@namespace/conda_environments/cpu/<operator-name>/<operator-version>/<operator-name>_<operator-version>


3. Adjust the ``<operator-name>.yaml`` configuration with the proper input/output folders. When running the operator in a Data Flow application, it won't have access to local folders, so input data and output folders should be placed in the Object Storage bucket.

4. Run the operator on the Data Flow service using this command::

.. code-block:: bash

    ads operator run -f <operator-name>.yaml --backend-config backend_dataflow_dataflow_config.yaml

5. Monitor the logs using the ``ads opctl watch`` command::

.. code-block:: bash

    ads opctl watch <OCID>
