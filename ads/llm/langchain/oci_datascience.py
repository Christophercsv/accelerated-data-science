#!/usr/bin/env python
# -*- coding: utf-8 -*--

# Copyright (c) 2024 Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl/

import logging
import re
from functools import wraps
from typing import Any, Callable, Dict, List, Optional, Protocol, Union

from langchain_core.callbacks import CallbackManagerForLLMRun
from langchain_core.language_models.llms import BaseLLM
from langchain_core.outputs import Generation, LLMResult
from langchain_core.pydantic_v1 import Extra, Field, root_validator
from langchain_core.utils import get_from_dict_or_env
from packaging import version

logger = logging.getLogger(__name__)

OCI_LLM_ENDPOINT = "OCI_LLM_ENDPOINT"
MIN_ADS_VERSION = "2.9.1"  # "2.11.13"


class UnsupportedAdsVersionError(Exception):
    def __init__(self, current_version: str, required_version: str):
        super().__init__(
            f"The `ads` version {current_version} currently installed is incompatible with "
            "the `langchain` version in use. To resolve this issue, please upgrade to `ads` "
            f"version {required_version} or later using the command: `pip install oracle-ads -U`"
        )


class InferenceBackend(Protocol):
    """Protocol for the inference backend."""

    def _generate(
        self,
        prompts: List[str],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> LLMResult:
        """Run the LLM on the given prompts."""

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Get the identifying parameters."""
        ...


def _validate_dependency(func: Callable[..., Any]) -> Callable[..., Any]:
    """
    Decorator to validate the presence and version of the `ads` package.

    Raises:
        ImportError: If the `ads` package is not installed.
        UnsupportedAdsVersionError: If the installed `ads` version is lower than the required version.
    """

    @wraps(func)
    def wrapper(*args, **kwargs) -> Any:
        try:
            from ads import __version__ as ads_version

            if version.parse(ads_version) < version.parse(MIN_ADS_VERSION):
                raise UnsupportedAdsVersionError(ads_version, MIN_ADS_VERSION)

        except ImportError as ex:
            raise ImportError(
                "Could not import `ads` python package. "
                "Please install it with `pip install oracle-ads`."
            ) from ex
        return func(*args, **kwargs)

    return wrapper


def _is_hex_string(data: str) -> bool:
    """
    Check if the provided string is a valid hexadecimal string.

    Args:
        data (str): The string to check.

    Returns:
        bool: True if the string is a valid hexadecimal string, False otherwise.
    """
    if not isinstance(data, str):
        return False
    hex_pattern = r"^[0-9a-fA-F]+$"
    return bool(re.match(hex_pattern, data))


def _deserialize_function_from_hex(
    hex_data: str, allow_unsafe_deserialization: Optional[bool] = False
) -> Callable[..., Any]:
    """
    Deserialize a pickled function from a hexadecimal string.

    Args:
        hex_data (str): The hexadecimal string to deserialize.
        allow_unsafe_deserialization (Optional[bool]): Whether to allow unsafe deserialization.

    Returns:
        Callable[..., Any]: The deserialized function.

    Raises:
        ValueError: If deserialization is not allowed or fails.
        ImportError: If the `cloudpickle` package is not installed.
    """
    if not allow_unsafe_deserialization:
        raise ValueError(
            "Deserialization requires opting in to allow unsafe deserialization. "
            "Set allow_unsafe_deserialization=True to proceed. "
            "Be aware that deserializing untrusted data can execute arbitrary code."
        )

    try:
        import cloudpickle
    except ImportError as e:
        raise ImportError("Please install cloudpickle>=2.0.0. Error: {e}")

    try:
        return cloudpickle.loads(bytes.fromhex(hex_data))
    except Exception as e:
        raise ValueError(f"Failed to deserialize function from hex string. Error: {e}")


class OCIModelDeployment(BaseLLM):
    """OCI Data Science Model Deployment."""

    auth: Dict[str, Any] = Field(default_factory=dict, exclude=True)
    """
    The authentication dictionary used for OCI API requests. Default is an empty dictionary.
    If not provided, it will be autogenerated based on the environment variables.
    For more details, refer to:
    https://accelerated-data-science.readthedocs.io/en/latest/user_guide/cli/authentication.html.
    """

    endpoint: Optional[str] = None
    """The URI of the endpoint from the deployed model."""

    inference_framework: Optional[str] = "vllm"
    """
    The framework used for inference. Examples include 'vllm', 'tgi', 'generic', 'llama.cpp'.
    Use `OCIModelDeployment.supported_frameworks()` to see the list of supported frameworks.
    The `vllm` is used by default.
    """

    model_kwargs: Optional[Dict[str, Any]] = Field(default_factory=dict)
    """
    Additional keyword arguments for the model. The default values will be taken based on the
    selected framework. Use this variable to pass model sampling parameters.
    Use `OCIModelDeployment.help("vllm")` to get details about the particular framework.
    """

    allow_unsafe_deserialization: bool = False
    """
    Flag to allow unsafe deserialization of pickled functions. Default is False.
    This will be used to deserialize `transform_input_fn` and `transform_output_fn` if provided.

    Important: Data can be compromised by a malicious actor if not handled properly, including
    a malicious payload that, when deserialized with pickle, can execute arbitrary code on your machine.
    """

    transform_input_fn: Optional[Union[str, Callable[..., Dict]]] = None
    """
    Function to convert `{prompt, stop, **model_kwargs}` into a JSON-compatible request object that is accepted by the endpoint.
    By default, the one implemented at the framework level will be used. However, if the
    default behavior needs to be changed, this function can be used.

    Important: This method will de serialized with cloudpickle.
    Data can be compromised by a malicious actor if not handled properly, including
    a malicious payload that, when deserialized with pickle, can execute arbitrary code on your machine.
    """

    transform_output_fn: Optional[Union[str, Callable[..., List[Generation]]]] = None
    """
    Function to transform the response from the endpoint to the `List[Generation]` before returning it.
    By default, the one implemented at the framework level will be used. However, if the
    default behavior needs to be changed, this function can be used.

    Important: This method will de serialized with cloudpickle.
    Data can be compromised by a malicious actor if not handled properly, including
    a malicious payload that, when deserialized with pickle, can execute arbitrary code on your machine.
    """

    _backend: InferenceBackend = Field(default=None, exclude=True)
    """
    The backend interface for the inference model.
    It will be created based on the selected inference framework.
    """

    class Config:
        extra = Extra.forbid
        # underscore_attrs_are_private = True

    @staticmethod
    @_validate_dependency
    def supported_frameworks() -> List[str]:
        """
        Get the list of supported inference frameworks.

        Returns:
            List[str]: The supported inference frameworks.
        """
        from ads.llm.langchain.inference_backend import InferenceBackendFactory

        return InferenceBackendFactory.supported_frameworks()

    @root_validator()
    @_validate_dependency
    def validate_environment(cls, values: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validate and set up the environment.

        Args:
            values (Dict[str, Any]): The input values.

        Returns:
            Dict[str, Any]: The validated and possibly modified values.
        """
        from ads.common.auth import default_signer
        from ads.llm.langchain.inference_backend import InferenceBackendFactory

        values["auth"] = values.get("auth") or default_signer()
        values["endpoint"] = get_from_dict_or_env(values, "endpoint", OCI_LLM_ENDPOINT)

        if values.get("transform_input_fn") and _is_hex_string(
            values["transform_input_fn"]
        ):
            values["transform_input_fn"] = _deserialize_function_from_hex(
                hex_data=values["transform_input_fn"],
                allow_unsafe_deserialization=values.get("allow_unsafe_deserialization"),
            )
        if values.get("transform_output_fn") and _is_hex_string(
            values["transform_output_fn"]
        ):
            values["transform_output_fn"] = _deserialize_function_from_hex(
                hex_data=values["transform_output_fn"],
                allow_unsafe_deserialization=values.get("allow_unsafe_deserialization"),
            )

        # setup backend
        values["_backend"] = InferenceBackendFactory.get_backend(
            values.get("inference_framework")
        )(**values)

        return values

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """
        Get the identifying parameters of the backend.

        Returns:
            Dict[str, Any]: The identifying parameters.
        """

        return {
            "inference_framework": self.inference_framework,
            **self._backend._identifying_params,
        }

    def _generate(
        self,
        prompts: List[str],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> LLMResult:
        """
        Run the LLM on the given prompts and input.

        Args:
            prompts (List[str]): The list of prompts to process.
            stop (Optional[List[str]]): Optional stop words.
            run_manager (Optional[CallbackManagerForLLMRun]): Optional run manager.
            kwargs (Any): Additional keyword arguments.

        Returns:
            LLMResult: The result from the backend.
        """
        return self._backend._generate(
            prompts=prompts, stop=stop, run_manager=run_manager, **kwargs
        )

    @property
    def _llm_type(self) -> str:
        return "oci_model_deployment"

    @classmethod
    @_validate_dependency
    def help(cls, inference_framework: Optional[str] = None) -> None:
        """Provides comprehensive information about each inference framework."""
        from ads.llm.langchain.inference_backend import InferenceBackendFactory

        InferenceBackendFactory.help(framework=inference_framework)
